{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying suicidal information on social media "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project using a classification technique to classify suicide vs non-suicide information using social media data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up the data \n",
    "data was generated from twitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "#from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import itertools\n",
    "from sklearn.externals import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The best part of fall is giving candu out to l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Motorhead- White Line Fever, The Watcher, Don'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I absolutely hate people. \\nWe arenâ€™t better...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I remember listening to sleeping w sirens bett...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good night boys , your moms and I are going up...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Another lazy tweet. Does one of you type while...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>And my response is yes it will.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Oh there you are. Now take a step back my hair...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Better Off Dead   You had better shave her  vi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Go back to your baloney bopping my friend.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  Sentiment\n",
       "0  The best part of fall is giving candu out to l...          0\n",
       "1  Motorhead- White Line Fever, The Watcher, Don'...          1\n",
       "2  I absolutely hate people. \\nWe arenâ€™t better...          1\n",
       "3  I remember listening to sleeping w sirens bett...          1\n",
       "4  Good night boys , your moms and I are going up...          0\n",
       "5  Another lazy tweet. Does one of you type while...          0\n",
       "6                    And my response is yes it will.          0\n",
       "7  Oh there you are. Now take a step back my hair...          1\n",
       "8  Better Off Dead   You had better shave her  vi...          1\n",
       "9         Go back to your baloney bopping my friend.          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"Tweets\", \"Sentiment\"]\n",
    "df = pd.read_csv(r'social_data.csv', names=columns )\n",
    "#add header column titles \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>202.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.846535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.361331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentiment\n",
       "count  202.000000\n",
       "mean     0.846535\n",
       "std      0.361331\n",
       "min      0.000000\n",
       "25%      1.000000\n",
       "50%      1.000000\n",
       "75%      1.000000\n",
       "max      1.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n",
    "#df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing data\n",
    "#df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The best part of fall is giving candu out to l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Motorhead White Line Fever The Watcher Dont Li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I absolutely hate people We arent better off d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I remember listening to sleeping w sirens bett...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good night boys  your moms and I are going up ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Another lazy tweet Does one of you type while ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>And my response is yes it will</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Oh there you are Now take a step back my hairy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Better Off Dead   You had better shave her  vi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Go back to your baloney bopping my friend</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  Sentiment\n",
       "0  The best part of fall is giving candu out to l...          0\n",
       "1  Motorhead White Line Fever The Watcher Dont Li...          1\n",
       "2  I absolutely hate people We arent better off d...          1\n",
       "3  I remember listening to sleeping w sirens bett...          1\n",
       "4  Good night boys  your moms and I are going up ...          0\n",
       "5  Another lazy tweet Does one of you type while ...          0\n",
       "6                     And my response is yes it will          0\n",
       "7  Oh there you are Now take a step back my hairy...          1\n",
       "8  Better Off Dead   You had better shave her  vi...          1\n",
       "9          Go back to your baloney bopping my friend          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove special characters \n",
    "df['Tweets'] = df['Tweets'].map(lambda x: re.sub(r'[^A-Za-z0-9 ]+', '', x))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore the data, word cloud viz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141                          Now Im just better off dead\n",
       "29     IM BETTER OFF DEADIM BETTER OFF DEADIM BETTER OFF\n",
       "152    I Done Too Much For People Who Would Never Do ...\n",
       "135                                 Better off dead Soon\n",
       "9              Go back to your baloney bopping my friend\n",
       "133    this type of shit proves that i would be bette...\n",
       "4      Good night boys  your moms and I are going up ...\n",
       "36     Mommy going to bring you some spaghettios down...\n",
       "55     You were my everythingthoughts of a wedding ri...\n",
       "37     theyre still being tortured and not able to mo...\n",
       "65     Porque  que betterOffdead dos flatbush ainda n...\n",
       "111                                      Better off dead\n",
       "146    MONEY DONT HELP WITH THE PAIN WHERE TF THE DRU...\n",
       "102    fuck no Im better off dead then staying at hom...\n",
       "67                Im better off dead Im losing my house \n",
       "162    Diane Franklin  httpmodsnapradiocom  Diane Fra...\n",
       "35     Over My Head Better Off DeadSum 41 httpwwwyout...\n",
       "103                                 Am i better off dead\n",
       "90     When the voice in your head says youre better ...\n",
       "20     Being depressed like this is so fucking boring...\n",
       "19     radiohead is not a depressive band is THE depr...\n",
       "42     its simple its so damn simple i am a mistake i...\n",
       "72     Anyone else thinking of Better Off Dead Two do...\n",
       "23              You Ever Just Wanna Fucking Off Yourself\n",
       "13                                       Better off dead\n",
       "101    If you have racist and ignorant parents and gr...\n",
       "2      I absolutely hate people We arent better off d...\n",
       "157       Id Rather Smoke Cigs Than Them Weird Ass Vapes\n",
       "44                            maybe i am better off dead\n",
       "10                                     Better off dead  \n",
       "                             ...                        \n",
       "163    Hes 9 He gets bullied He has thoughts of suici...\n",
       "92                                       better off dead\n",
       "110                   I think I would be better off dead\n",
       "116    And he was in Risky Business Better Off Dead a...\n",
       "25      Thank you thank you very muchWe do 2 shows a day\n",
       "144     Classic Better off Dead  pictwittercom0izGXSnILg\n",
       "58     Very cool I loved the movie Better Off Dead si...\n",
       "3      I remember listening to sleeping w sirens bett...\n",
       "115    Id be better off deadThan to live without you ...\n",
       "159    More New ssgsplurge1 Prod 1chaibeats capcrunch...\n",
       "189    Girlfriend is Better off DeadefoxbandKaceyecaK...\n",
       "155    You know what John Cusack film NEVER gets ment...\n",
       "183    Kicked off 80s movie marathon weekend part 2 w...\n",
       "172                                      Better off DEAD\n",
       "45     I got a bar to play Everybody Wants Some by Va...\n",
       "88     Better off dead than red  pictwittercomz7U0lT2Wv6\n",
       "64     Theres Voices In My Head Telling Me Im Better ...\n",
       "14                          That say Im better off DEAD \n",
       "176     like life is trying hardest to say every year...\n",
       "28           sometimes i feel like id be better off dead\n",
       "118    When the voice in your head Says youre better ...\n",
       "136    This song is frickin great STGxBot Better Off ...\n",
       "175    Dont say Im better off deadCause heavens full ...\n",
       "128    maybe im better off deadif i was would it fina...\n",
       "161    10 movies to know me1 The Fifth Element2 Princ...\n",
       "132    Anyone who supports Babu at this point is bett...\n",
       "53     day  after deciding id be better off dead i dk...\n",
       "171    So instead of getting credit for saving this c...\n",
       "106    Im not okNo solution 2 get thoughts out of my ...\n",
       "7      Oh there you are Now take a step back my hairy...\n",
       "Name: Tweets, Length: 161, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#divide the data into training and test \n",
    "train_tweets, test_tweets, train_targets, test_targets = train_test_split(df[\"Tweets\"], \n",
    "                                                                              df[\"Sentiment\"],\n",
    "                                                                              test_size=0.2)\n",
    "train_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.02 ms\n",
      "Wall time: 3.98 ms\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n",
      "Classifier: LogisticRegression(C=0.0001, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score:0.6829268292682927\n",
      "\n",
      "Wall time: 2 ms\n",
      "Wall time: 9.97 ms\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n",
      "Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "                     weights='uniform')\n",
      "Accuracy score:0.6829268292682927\n",
      "\n",
      "Wall time: 2 ms\n",
      "Wall time: 11 ms\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n",
      "Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=9, p=2,\n",
      "                     weights='uniform')\n",
      "Accuracy score:0.6829268292682927\n",
      "\n",
      "Wall time: 2.99 ms\n",
      "Wall time: 12 ms\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n",
      "Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=27, p=2,\n",
      "                     weights='uniform')\n",
      "Accuracy score:0.6829268292682927\n",
      "\n",
      "Wall time: 44.9 ms\n",
      "Wall time: 1.99 ms\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n",
      "Classifier: SVC(C=0.025, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=True, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "Accuracy score:0.6829268292682927\n",
      "\n",
      "Wall time: 2 ms\n",
      "Wall time: 0 ns\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n",
      "Classifier: LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "Accuracy score:0.6585365853658537\n",
      "\n",
      "Wall time: 5.99 ms\n",
      "Wall time: 0 ns\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n",
      "Classifier: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "Accuracy score:0.6829268292682927\n",
      "\n",
      "Wall time: 15 ms\n",
      "Wall time: 998 µs\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n",
      "Classifier: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=2018,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score:0.6829268292682927\n",
      "\n",
      "Wall time: 6.98 ms\n",
      "Wall time: 997 µs\n",
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n",
      "Classifier: GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "Accuracy score:0.6097560975609756\n",
      "\n",
      "Wall time: 997 µs"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 0 ns\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)\n",
      "Classifier: LogisticRegression(C=0.0001, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score:0.6829268292682927\n",
      "\n",
      "Wall time: 3.99 ms\n",
      "Wall time: 9.97 ms\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)\n",
      "Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "                     weights='uniform')\n",
      "Accuracy score:0.6829268292682927\n",
      "\n",
      "Wall time: 2.99 ms\n",
      "Wall time: 9.97 ms\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)\n",
      "Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=9, p=2,\n",
      "                     weights='uniform')\n",
      "Accuracy score:0.6829268292682927\n",
      "\n",
      "Wall time: 1.99 ms\n",
      "Wall time: 11 ms\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)\n",
      "Classifier: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=27, p=2,\n",
      "                     weights='uniform')\n",
      "Accuracy score:0.6829268292682927\n",
      "\n",
      "Wall time: 40.9 ms\n",
      "Wall time: 2 ms\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)\n",
      "Classifier: SVC(C=0.025, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=True, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "Accuracy score:0.6829268292682927\n",
      "\n",
      "Wall time: 997 µs\n",
      "Wall time: 0 ns\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)\n",
      "Classifier: LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "Accuracy score:0.6829268292682927\n",
      "\n",
      "Wall time: 5.98 ms\n",
      "Wall time: 997 µs\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)\n",
      "Classifier: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n",
      "Accuracy score:0.6829268292682927\n",
      "\n",
      "Wall time: 15 ms\n",
      "Wall time: 997 µs\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)\n",
      "Classifier: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=2018,\n",
      "                       verbose=0, warm_start=False)\n",
      "Accuracy score:0.6829268292682927\n",
      "\n",
      "Wall time: 3.99 ms\n",
      "Wall time: 0 ns\n",
      "Vectorizer: TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)\n",
      "Classifier: GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "Accuracy score:0.6097560975609756\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Vectorization \n",
    "\n",
    "# Declare vectorizers to be used\n",
    "VECTORIZERS = (\n",
    "    CountVectorizer(),\n",
    "    TfidfVectorizer(),\n",
    ")\n",
    "\n",
    "\n",
    "# Declare classifiers to be used\n",
    "CLASSIFIERS = (\n",
    "    LogisticRegression(C=10e-5, solver=\"liblinear\", max_iter=10000),\n",
    "    KNeighborsClassifier(3),\n",
    "    KNeighborsClassifier(9),\n",
    "    KNeighborsClassifier(27),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    LinearSVC(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(random_state=2018),\n",
    "    GaussianNB(),\n",
    ")\n",
    "\n",
    "for vectorizer, classifier in itertools.product(VECTORIZERS, CLASSIFIERS):\n",
    "    # Vectorize preprocessed sentences\n",
    "    train_features = vectorizer.fit_transform(train_tweets)\n",
    "    \n",
    "\n",
    "    # Train the model\n",
    "    %time fit = classifier.fit(train_features.toarray(), train_targets)\n",
    "    \n",
    "\n",
    "    # Check the accuracy of the model on test data and display it\n",
    "    test_features = vectorizer.transform(test_tweets)\n",
    "    %time test_predictions = fit.predict(test_features.toarray())\n",
    "    accuracy = accuracy_score(test_predictions, test_targets)\n",
    "    print(\"Vectorizer: {}\\nClassifier: {}\\nAccuracy score:{}\\n\".format(vectorizer,\n",
    "                                                                       classifier, \n",
    "                                                                       accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer: CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n",
      "Classifier: LogisticRegression(C=0.0001, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Accuracy score:0.6097560975609756\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model3.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#countvectorizer and logistic regression have a high accuracy score, so we would use it for the demo, you can use any\n",
    "cv = CountVectorizer()\n",
    "clf = LogisticRegression(C=10e-5, solver=\"liblinear\", max_iter=10000)\n",
    "train_fe = cv.fit_transform(train_tweets)\n",
    "fit_ = clf.fit(train_fe.toarray(), train_targets)\n",
    "\n",
    "test_ = cv.transform(test_tweets)\n",
    "test_pred = fit.predict(test_.toarray())\n",
    "accuracy = accuracy_score(test_pred, test_targets)\n",
    "print(\"Vectorizer: {}\\nClassifier: {}\\nAccuracy score:{}\\n\".format(cv,\n",
    "                                                                    clf, \n",
    "                                                                       accuracy))\n",
    "joblib.dump(cv, 'vectorizer.pkl')\n",
    "joblib.dump(fit_, 'model3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "data = [\"we are happy\"]\n",
    "vect = cv.transform(data).toarray()\n",
    "print(fit.predict(vect))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
